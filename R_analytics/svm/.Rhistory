library(polycor)
#Clear old data
rm(list=ls())
#use filechooser to select csv file
mydata <- read.csv(file.choose(), sep = ',')
#Prepare data
mydata.orig = mydata #save orig data copy
mydata <- na.omit(mydata) # listwise deletion of missing
#mydata[1] <- NULL #remove the first column
#mydata <- scale(mydata) #scale data if needed
# Determine Number of Factors to Extract
corinfo <- mydata[,1:5]
ev <- eigen(cor(corinfo)) # get eigenvalues
ap <- parallel(subject=nrow(corinfo),var=ncol(corinfo), rep=100, cent=.05)
nS <- nScree(x=ev$values, aparallel=ap$eigen$qevpea)
plotnScree(nS)
set.seed(1)
faPC <- fa(r = cor(corinfo), nfactors =
n.factors, rotate = "varimax", fm = "pa")
plot(faPC,labels=names(mydata),cex=.7, ylim=c(-.1,1))
summary(faPC)
#visualize
factor.plot(faPC, cut=0.3)
#diagram path
fa.diagram(faPC)
#libraries
library(psy)
library(nFactors)
library(MASS)
library(psych)
library(cluster)
library(ggplot2)
library(factoextra)
library(corrplot)
library(lattice)
library(readxl)
library(gridExtra)
library(dplyr)
library(tidyr)
library(ggpubr)
library(polycor)
# Determine Number of Factors to Extract
corinfo <- mydata[,1:5]
ev <- eigen(cor(corinfo)) # get eigenvalues
ap <- parallel(subject=nrow(corinfo),var=ncol(corinfo), rep=100, cent=.05)
nS <- nScree(x=ev$values, aparallel=ap$eigen$qevpea)
plotnScree(nS)
set.seed(1)
n.factors = 2
faPC <- fa(r = cor(corinfo), nfactors =
n.factors, rotate = "varimax", fm = "pa")
plot(faPC,labels=names(mydata),cex=.7, ylim=c(-.1,1))
summary(faPC)
#visualize
factor.plot(faPC, cut=0.3)
#diagram path
fa.diagram(faPC)
View(mydata)
fa.parallel(corinfo)
vss(corinfo, n.obs=4, rotate="varimax")  # very simple structure
pairs(data=wine_data,
main="Simple Scatterplot Matrix")
wine_data <- read.csv(file.choose(), sep = ',')
wine_data <- read.csv(file.choose(), sep = ',')
pairs(data=wine_data,
main="Simple Scatterplot Matrix")
pairs(~,data=wine_data,
main="Simple Scatterplot Matrix")
pairs(~wine_data,data=wine_data,
main="Simple Scatterplot Matrix")
#scatterplots
pairs(~wine_data[1:17],data=wine_data,
main="Simple Scatterplot Matrix")
pairs(~Chem1+Chem2,data=wine_data,
main="Simple Scatterplot Matrix")
corinfo <- wine_data[,2:14]
fit_pca <- princomp(cor(corinfo), cor=TRUE)
summary(fit_pca) # print variance accounted for
loadings(fit_pca) # pc loadings
plot(fit_pca,type="lines") # scree plot
#fit_pca$scores # the principal components
biplot(fit_pca)
#Perceptual map
pc.cr <-princomp(corinfo, cor = TRUE) #scale data
summary(pc.cr)
biplot(pc.cr)
A <- data.frame(Gender = c("F", "F", "M", "F", "B", "M", "M"), Height = c(154, 167, 178, 145, 169, 183, 176))
View(A)
A <- data.frame(Gender = c("F", "F", "M", "F", "B", "M", "M"),
Height = c(154, 167, 178, 145, 169, 183, 176))
#recoding
A[,1] <- ifelse(A[,1] == "M", 1, ifelse(A[,1] == "F", 2, 99))
View(A)
library(forecast)
library(tseries)
install.packages("forecast","tseries")
install.packages("forecast")
install.packages("tseries")
library(ggplot2)
library(forecast)
library(tseries)
#arima modeling
#arima cheat sheet
# Examine your data
# Plot the data and examine its patterns and irregularities
# Clean up any outliers or missing values if needed
# tsclean() is a convenient method for outlier removal and inputing missing values
# Take a logarithm of a series to help stabilize a strong growth trend
# Decompose your data
# Does the series appear to have trends or seasonality?
# Use decompose() or stl() to examine and possibly remove components of the series
# Stationarity
# Is the series stationary?
# Use adf.test(), ACF, PACF plots to determine order of differencing needed
# Autocorrelations and choosing model order
# Choose order of the ARIMA by examining ACF and PACF plots
# Fit an ARIMA model
# Evaluate and iterate
# Check residuals, which should haven no patterns and be normally distributed
# If there are visible patterns or bias, plot ACF/PACF. Are any additional order parameters needed?
# Refit model if needed. Compare model errors and fit criteria such as AIC or BIC.
#load data
#daily_data = read.csv('day.csv', header=TRUE, stringsAsFactors=FALSE)
daily_data = read.csv(file.choose(), sep = ',', header=TRUE, stringsAsFactors=FALSE)
daily_data$Date = as.Date(daily_data$dteday)
ggplot(daily_data, aes(Date, cnt)) + geom_line() + scale_x_date('month')  + ylab("Daily Bike Checkouts") +
xlab("")
daily_data.orig = daily_data
#clean data since there are large spikes in daily checkouts
count_ts = ts(daily_data[, c('cnt')])
daily_data$clean_cnt = tsclean(count_ts)
ggplot() +
geom_line(data = daily_data, aes(x = Date, y = clean_cnt)) + ylab('Cleaned Bicycle Count')
# moving average
daily_data$cnt_ma = ma(daily_data$clean_cnt, order=7) # using the clean count with no outliers
daily_data$cnt_ma30 = ma(daily_data$clean_cnt, order=30)
ggplot() +
geom_line(data = daily_data, aes(x = Date, y = clean_cnt, colour = "Counts")) +
geom_line(data = daily_data, aes(x = Date, y = cnt_ma,   colour = "Weekly Moving Average"))  +
geom_line(data = daily_data, aes(x = Date, y = cnt_ma30, colour = "Monthly Moving Average"))  +
ylab('Bicycle Count')
count_ma = ts(na.omit(daily_data$cnt_ma), frequency=30)
decomp = stl(count_ma, s.window="periodic")
deseasonal_cnt <- seasadj(decomp)
plot(decomp)
adf.test(count_ma, alternative = "stationary")
Acf(count_ma, main='')
Pacf(count_ma, main='')
count_d1 = diff(deseasonal_cnt, differences = 1)
plot(count_d1)
adf.test(count_d1, alternative = "stationary")
Acf(count_d1, main='ACF for Differenced Series')
Pacf(count_d1, main='PACF for Differenced Series')
auto.arima(deseasonal_cnt, seasonal=FALSE)
Series: deseasonal_cnt
ARIMA(1,1,1)
library(forecast)
library(tseries)
#model validation
fit<-auto.arima(deseasonal_cnt, seasonal=FALSE)
tsdisplay(residuals(fit), lag.max=45, main='(1,1,1) Model Residuals')
fit2 = arima(deseasonal_cnt, order=c(1,1,7))
fit2
tsdisplay(residuals(fit2), lag.max=15, main='Seasonal Model Residuals')
fcast <- forecast(fit2, h=30)
plot(fcast)
#model performance
hold <- window(ts(deseasonal_cnt), start=700)
fit_no_holdout = arima(ts(deseasonal_cnt[-c(700:725)]), order=c(1,1,7))
fcast_no_holdout <- forecast(fit_no_holdout,h=25)
plot(fcast_no_holdout, main=" ")
lines(ts(deseasonal_cnt))
fit_w_seasonality = auto.arima(deseasonal_cnt, seasonal=TRUE)
fit_w_seasonality
seas_fcast <- forecast(fit_w_seasonality, h=30)
plot(seas_fcast)
library(forecast)
library(fpp2)
install.packages("fpp2")
mydata_hist <- mydata %>% filter(mydata$left==1)
#libraries
library(psy)
library(nFactors)
library(MASS)
library(psych)
library(cluster)
library(ggplot2)
library(factoextra)
library(corrplot)
library(lattice)
library(readxl)
library(gridExtra)
library(dplyr)
library(tidyr)
library(ggpubr)
library(polycor)
hist(mydata_hist$last_evaluation,col="#FC4E07",
main = "Last evaluation", xlab="") + theme_minimal()
mydata_hist <- mydata %>% filter(mydata$left==1)
hist(mydata_hist$last_evaluation,col="#FC4E07",
main = "Last evaluation", xlab="") + theme_minimal()
col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
corrplot(cor(mydata_hist), method="color", col=col(200),
type="full",
addCoef.col = "black", # Add coefficient of correlation
tl.col="black", tl.srt=45, #Text label color and rotation
# hide correlation coefficient on the principal diagonal
diag=TRUE
)
#correlation
corinfo <- mydata[,1:8]
corrplot(cor(corinfo))
corrplot(cor(corinfo), method="number")
#write.csv(corinfo, file="correlation.csv")
col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
corrplot(cor(corinfo), method="color", col=col(200),
type="full",
addCoef.col = "black", # Add coefficient of correlation
tl.col="black", tl.srt=45, #Text label color and rotation
# hide correlation coefficient on the principal diagonal
diag=TRUE
col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
corrplot(cor(mydata_hist[,1:8]), method="color", col=col(200),
type="full",
addCoef.col = "black", # Add coefficient of correlation
tl.col="black", tl.srt=45, #Text label color and rotation
# hide correlation coefficient on the principal diagonal
diag=TRUE
)
col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
corrplot(cor(mydata_hist[,1:6,8]), method="color", col=col(200),
type="full",
addCoef.col = "black", # Add coefficient of correlation
tl.col="black", tl.srt=45, #Text label color and rotation
# hide correlation coefficient on the principal diagonal
diag=TRUE
)
#correlation those who stayed
mydata_hist1 <- mydata %>% filter(mydata$left==0)
corrplot(cor(mydata_hist[,1:6,8]), method="color", col=col(200),
type="full",
addCoef.col = "black", # Add coefficient of correlation
tl.col="black", tl.srt=45, #Text label color and rotation
# hide correlation coefficient on the principal diagonal
diag=TRUE
)
library(corrplot)
#correlation those who stayed
mydata_hist1 <- mydata %>% filter(mydata$left==0)
corrplot(cor(mydata_hist[,1:6,8]), method="color", col=col(200),
type="full",
addCoef.col = "black", # Add coefficient of correlation
tl.col="black", tl.srt=45, #Text label color and rotation
# hide correlation coefficient on the principal diagonal
diag=TRUE
)
#libraries
library(psy)
library(nFactors)
library(MASS)
library(psych)
library(cluster)
library(ggplot2)
library(factoextra)
library(corrplot)
library(lattice)
library(readxl)
library(gridExtra)
library(dplyr)
library(tidyr)
library(ggpubr)
library(polycor)
#correlation those who stayed
mydata_hist1 <- mydata %>% filter(mydata$left==0)
corrplot(cor(mydata_hist[,1:6,8]), method="color", col=col(200),
type="full",
addCoef.col = "black", # Add coefficient of correlation
tl.col="black", tl.srt=45, #Text label color and rotation
# hide correlation coefficient on the principal diagonal
diag=TRUE
)
hist(mydata_hist$Work_accident,col="#3090C7",
main = "Work accident", xlab="")
ggplot(mydata_hist,aes(x=sales))+geom_bar(fill="#FF00FF") +
xlab("Department")
library(ggplot2)
ggplot(mydata_hist,aes(x=sales))+geom_bar(fill="#FF00FF") +
xlab("Department")
plot(mydata_hist$salary,col="#009999", main = "Salary", xlab="") + theme_minimal()
ggplot(mydata_hist,aes(x=sales))+geom_bar(fill="#FF00FF") +
xlab("Department") + theme_classic()
ggplot(mydata_hist,aes(x=sales))+geom_bar(fill="#FF00FF") +
xlab("Department") + theme_dark()
ggplot(mydata_hist,aes(x=sales))+geom_bar(fill="#FF00FF") +
xlab("Department") + theme_grey()
library(ggplot2)
library(psy)
library(nFactors)
library(MASS)
library(psych)
library(cluster)
library(ggplot2)
library(factoextra)
library(corrplot)
library(lattice)
library(readxl)
library(gridExtra)
library(dplyr)
library(tidyr)
library(ggpubr)
library(polycor)
#==========================
#logistic regression
#==========================
#Clear old data
rm(list=ls())
#use filechooser to select csv file
mydata <- read.csv(file.choose(), sep = ',')
#Prepare data
mydata.orig = mydata #save orig data copy
mydata <- na.omit(mydata) # listwise deletion of missing
#mydata[1] <- NULL #remove the first column
#mydata <- scale(mydata) #scale data if needed
#set seed
set.seed(1)
#convert department and salary to factors
mydata$sales<-as.factor(data$sales)
mydata$salary<-as.factor(data$salary)
mydata$salary<-ordered(data$salary,levels=c("low","medium","high"))
#set seed
set.seed(1)
#convert department and salary to factors
mydata$sales<-as.factor(data$sales)
mydata$salary<-as.factor(data$salary)
mydata$salary<-ordered(data$salary,levels=c("low","medium","high"))
#set seed
set.seed(1)
#convert department and salary to factors
mydata$sales<-as.factor(mydata$sales)
mydata$salary<-as.factor(mydata$salary)
mydata$salary<-ordered(mydata$salary,levels=c("low","medium","high"))
install.packages("LogicReg")
install.packages("LogicReg")
#logistic regression model
ctrl <- trainControl(method = "cv", number = 5)
modelglm <- train(as.factor(left) ~. , mydata = trainSplit, method = "glm", trControl = ctrl)
summary(modelglm)
#prediction
predictors <- names(trainSplit)[names(trainSplit) != 'left']
predglm <- predict(modelglm, testSplit)
summary(predglm)
#confusion matrix
confusionMatrix(predglm, testSplit$left)
library(LogicReg)
#logistic regression model
ctrl <- trainControl(method = "cv", number = 5)
modelglm <- train(as.factor(left) ~. , mydata = trainSplit, method = "glm", trControl = ctrl)
summary(modelglm)
#prediction
predictors <- names(trainSplit)[names(trainSplit) != 'left']
predglm <- predict(modelglm, testSplit)
summary(predglm)
#confusion matrix
confusionMatrix(predglm, testSplit$left)
#logistic regression model
ctrl <- trainControl(method = "cv", number = 5)
modelglm <- train(as.factor(left) ~. , mydata = trainSplit, method = "glm", trControl = ctrl)
summary(modelglm)
ctrl <- trainControl(method = "cv", number = 5)
set.seed(1234)
library(caret)
#training and test data
splitIndex <- createDataPartition(mydata$left, p = .80,list = FALSE, times = 1)
trainSplit <- mydata[ splitIndex,]
testSplit <- mydata[-splitIndex,]
print(table(trainSplit$left))
ctrl <- trainControl(method = "cv", number = 5)
modelglm <- train(as.factor(left) ~. , mydata = trainSplit, method = "glm", trControl = ctrl)
summary(modelglm)
modelglm <- train(as.factor(left) ~. , mydata = trainSplit, method = "glm", trControl = ctrl)
modelglm <- train(as.factor(left) ~. , data = trainSplit, method = "glm", trControl = ctrl)
summary(modelglm)
predictors <- names(trainSplit)[names(trainSplit) != 'left']
predglm <- predict(modelglm, testSplit)
summary(predglm)
confusionMatrix(predglm, testSplit$left)
#people that stayed
mydata_stay <- mydata %>% filter(mydata$left==0)
hr_model <- mydata %>% filter(mydata$left==0)
gmlmodel <- train(left~., data=hr_model, trControl=train_control, method="LogitBoost")
# make predictions
predictions<- predict(gmlmodel,hr_model)
gmlmodelbinded <- cbind(hr_model,predictions)
# summarize results
confusionMatrix<- confusionMatrix(gmlmodelbinded$predictions,gmlmodelbinded$left)
confusionMatrix
#train model
gmlmodel <- train(left~., data=hr_model, trControl=train_control, method="LogitBoost")
# make predictions
predictions<- predict(gmlmodel,hr_model)
#people that stayed
gp <- mydata %>% filter(mydata$left==0)
#training & test data
splitIndex <- createDataPartition(gp$left, p = .80, list = FALSE, times = 1)
trainSplit <- gp[splitIndex,]
testSplit <- gp[-splitIndex,]
print(table(trainSplit$left))
gp <- mydata %>% filter(mydata$left==0)
splitIndex <- createDataPartition(gp$left, p = .80, list = FALSE, times = 1)
hr_model <- mydata %>% filter(mydata$left==0)
gmlmodel <- train(left~., data=hr_model, trControl=train_control, method="LogitBoost")
predictions<- predict(gmlmodel,hr_model)
library(caret)
mydata_stay <- mydata %>% filter(mydata$left==1)
View(mydata_stay)
mydata_stay <- mydata %>% filter(mydata$left==0)
View(mydata_stay)
mydata_stay$sales<-as.factor(mydata_stay$sales)
mydata_stay$salary<-as.factor(mydata_stay$salary)
mydata_stay$salary<-ordered(mydata_stay$salary,levels=c("low","medium","high"))
splitIndex1 <- createDataPartition(mydata_stay$left, p = .80,list = FALSE, times = 1)
trainSplit1 <- mydata_stay[ splitIndex1,]
testSplit1 <- mydata_stay[-splitIndex1,]
print(table(trainSplit1$left))
set.seed(1234)
splitIndex1 <- createDataPartition(mydata_stay$left, p = .80,list = FALSE, times = 1)
View(mydata_stay)
#training and test data
splitIndex <- createDataPartition(mydata$left, p = .80,list = FALSE, times = 1)
trainSplit <- mydata[ splitIndex,]
testSplit <- mydata[-splitIndex,]
print(table(trainSplit$left))
#logistic regression model
ctrl <- trainControl(method = "cv", number = 5)
modelglm <- train(as.factor(left) ~. , data = trainSplit, method = "glm", trControl = ctrl)
summary(modelglm)
#prediction
predictors <- names(trainSplit)[names(trainSplit) != 'left']
predglm <- predict(modelglm, testSplit)
summary(predglm)
#confusion matrix
confusionMatrix(predglm, testSplit$left)
View(mydata_stay)
confusionMatrix(predglm, testSplit$left)
summary(modelglm)
#This file contains monthly average electricity usage data in Iowa City from 1971 to 1979.
#questions
#1.	Fit a seasonal trigonometric regression model to the electricity usage data.
#Forecast the electricity price for January 1980 using the trigonometric regression model.
#2.	Describe in detail two ways of obtaining the seasonally adjusted electricity usage data.
#3.	Obtain the 3rd order moving average for the electricity price data.
#What is the forecast for April 1980 based on this moving average.
#4.	Using the average electricity usage as the initial value,
#obtain the next 2 first order exponentially smoothed values using a weight Î¸ of 0.6.
#Show the detailed calculations step-by-step.
#libraries
rm(list=ls())
#use filechooser to select csv file
iowadata <- read.csv(file.choose(), sep = ',')
#Prepare data
iowadata.orig = iowadata #save orig data copy
iowadata <- na.omit(iowadata) # listwise deletion of missing
View(iowadata)
summary(iowadata)
getwd()
setwd("~/Documents/Coding/R/R_analytics/svm")
data <- read.csv('regression.csv', sep=",", header = TRUE)
# Plot the data
plot(data, pch=16)
# Create a linear regression model
model <- lm(Y ~ X, data)
# Add the fitted line
abline(model)
plot(data, pch=16)
model <- lm(Y ~ X , data)
# make a prediction for each X
predictedY <- predict(model, data)
# display the predictions
points(data$X, predictedY, col = "blue", pch=4)
rmse <- function(error)
{
sqrt(mean(error^2))
}
error <- model$residuals  # same as data$Y - predictedY
predictionRMSE <- rmse(error)
library(e1071)
model <- svm(Y ~ X , data)
predictedY <- predict(model, data)
points(data$X, predictedY, col = "red", pch=4)
error <- data$Y - predictedY
svrPredictionRMSE <- rmse(error)
#model tuning
# perform a grid search
tuneResult <- tune(svm, Y ~ X,  data = data,
ranges = list(epsilon = seq(0,1,0.1), cost = 2^(2:9))
)
print(tuneResult)
# Draw the tuning graph
plot(tuneResult)
#refine model and focus on darker region
tuneResult <- tune(svm, Y ~ X,  data = data,
ranges = list(epsilon = seq(0,0.2,0.01), cost = 2^(2:9))
)
print(tuneResult)
plot(tuneResult)
#select model
tunedModel <- tuneResult$best.model
tunedModelY <- predict(tunedModel, data)
error <- data$Y - tunedModelY
# this value can be different on your computer
# because the tune method  randomly shuffles the data
tunedModelRMSE <- rmse(error)
